{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functorch.dim import dims\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.ops import roi_align\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roi_Align 부분, 서현님 part\n",
    "def bilinear_interpolate(input, height, width, y, x, ymask, xmask):\n",
    "    \n",
    "    y = y.clamp(min=0)\n",
    "    x = x.clamp(min=0)\n",
    "    y_low = y.int()\n",
    "    x_low = x.int()\n",
    "    y_high = torch.where(y_low >= height - 1, height - 1, y_low + 1)\n",
    "    y_low = torch.where(y_low >= height - 1, height - 1, y_low)\n",
    "    y = torch.where(y_low >= height - 1, y.to(input.dtype), y)\n",
    "  \n",
    "    x_high = torch.where(x_low >= width - 1, width - 1, x_low + 1)\n",
    "    x_low = torch.where(x_low >= width - 1, width - 1, x_low)\n",
    "    x = torch.where(x_low >= width - 1, x.to(input.dtype), x)\n",
    "    \n",
    "    ly = y - y_low\n",
    "    lx = x - x_low\n",
    "    hy = 1. - ly\n",
    "    hx = 1. - lx\n",
    "    \n",
    "    def masked_index(y, x):\n",
    "        y = torch.where(ymask, y, 0)\n",
    "        x = torch.where(xmask, x, 0)\n",
    "        return input[y, x]\n",
    "\n",
    "    v1 = masked_index(y_low, x_low)\n",
    "    v2 = masked_index(y_low, x_high)\n",
    "    v3 = masked_index(y_high, x_low)\n",
    "    v4 = masked_index(y_high, x_high)\n",
    "    w1 = hy * hx\n",
    "    w2 = hy * lx\n",
    "    w3 = ly * hx\n",
    "    w4 = ly * lx\n",
    "\n",
    "    val = w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4\n",
    "    return val\n",
    "\n",
    "def roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n",
    "    _, _, height, width = input.size()\n",
    "\n",
    "    n, c, ph, pw = dims(4)\n",
    "    \n",
    "    ph.size = pooled_height\n",
    "    pw.size = pooled_width\n",
    "    offset_rois = rois[n]\n",
    "    roi_batch_ind = offset_rois[0].int()\n",
    "    offset = 0.5 if aligned else 0.0\n",
    "    roi_start_w = offset_rois[1] * spatial_scale - offset\n",
    "    roi_start_h = offset_rois[2] * spatial_scale - offset\n",
    "    roi_end_w = offset_rois[3] * spatial_scale - offset\n",
    "    roi_end_h = offset_rois[4] * spatial_scale - offset\n",
    "\n",
    "    roi_width = roi_end_w - roi_start_w\n",
    "    roi_height = roi_end_h - roi_start_h\n",
    "    if not aligned:\n",
    "        roi_width = torch.clamp(roi_width, min=1.0)\n",
    "        roi_height = torch.clamp(roi_height, min=1.0)\n",
    "\n",
    "    bin_size_h = roi_height / pooled_height\n",
    "    bin_size_w = roi_width / pooled_width\n",
    "\n",
    "    offset_input = input[roi_batch_ind][c]\n",
    "\n",
    "    roi_bin_grid_h = sampling_ratio if sampling_ratio > 0 else torch.ceil(roi_height / pooled_height)\n",
    "    roi_bin_grid_w = sampling_ratio if sampling_ratio > 0 else torch.ceil(roi_width / pooled_width)\n",
    "\n",
    "    count = torch.clamp(roi_bin_grid_h * roi_bin_grid_w, min=1)\n",
    "\n",
    "    iy, ix = dims(2)\n",
    "\n",
    "    iy.size = height  # < roi_bin_grid_h\n",
    "    ix.size = width  # < roi_bin_grid_w\n",
    "    \n",
    "    y = roi_start_h + ph * bin_size_h + (iy + 0.5) * bin_size_h / roi_bin_grid_h\n",
    "    x = roi_start_w + pw * bin_size_w + (ix + 0.5) * bin_size_w / roi_bin_grid_w\n",
    "    ymask = iy < roi_bin_grid_h\n",
    "    xmask = ix < roi_bin_grid_w\n",
    "    val = bilinear_interpolate(offset_input, height, width, y, x, ymask, xmask)\n",
    "    val = torch.where(ymask, val, 0)\n",
    "    val = torch.where(xmask, val, 0)\n",
    "    output = val.sum((iy, ix))\n",
    "    output /= count\n",
    "\n",
    "    return output.order(n, c, ph, pw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuremap을 얻기 위한 컨볼루션 부분\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool2d((28,28))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.avg_pooling(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 28, 28])\n",
      "tensor(490.0711, grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "simple_cnn = CNN()\n",
    "\n",
    "# coco 이미지 불러오기\n",
    "image_path = \"COCO_train2014_000000000030.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "#텐서 변환\n",
    "preprocess = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "# preprocess = transforms.Compose([\n",
    "#         transforms.Resize((224, 224)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "input_image = preprocess(image)\n",
    "input_image = input_image.unsqueeze(0)\n",
    "\n",
    "output_image = simple_cnn(input_image)\n",
    "\n",
    "\n",
    "# 임의의 피쳐맵 생성\n",
    "# featuremap은 cnn을 통과한 이미지의 output을 그대로 가져옴.\n",
    "# rois는 임의로 설정.\n",
    "features = output_image\n",
    "print(features.shape)\n",
    "rois = torch.tensor([\n",
    "    [0, 204,31,458,355]\n",
    "], dtype=torch.float)\n",
    "\n",
    "\n",
    "#roi를 통과한 output size 설정,\n",
    "#spatial_scale 은 28,28 사이즈를 7,7로 바꿔야하므로 4.0으로 변경\n",
    "output_size = (7, 7)\n",
    "spatial_scale = 1.0 / 4.0\n",
    "\n",
    "# Call the roi_align function\n",
    "pooled_features = roi_align(features, rois, spatial_scale, output_size[0], output_size[1], -1, False)\n",
    "\n",
    "print(pooled_features.sum())\n",
    "\n",
    "from torchvision.ops import roi_align as roi_align_torchvision\n",
    "\n",
    "print(roi_align_torchvision(features, rois, output_size, spatial_scale).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(pooled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SelectBackward0>)\n",
      "tensor([0.0254, 0.0259, 0.0264, 0.0269, 0.0276, 0.0287, 0.0298],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([6.4516e-05, 6.0186e-05, 5.5856e-05, 5.1526e-05, 4.5897e-05, 3.7237e-05,\n",
      "        2.8577e-05], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rois = pooled_features[0]\n",
    "height = pooled_features[0][2]\n",
    "weight = pooled_features[0][3]\n",
    "# print(rois)\n",
    "print(rois[0])\n",
    "print(height[0])\n",
    "print(weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스크부분 & sampadding은 메소드인 padding ='same'과 동일하다.\n",
    "\n",
    "class SamePad2d(nn.Module):\n",
    "    \"\"\"Mimics tensorflow's 'SAME' padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(SamePad2d, self).__init__()\n",
    "        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n",
    "        self.stride = torch.nn.modules.utils._pair(stride)\n",
    "\n",
    "    def forward(self, input):\n",
    "        in_width = input.size()[2]\n",
    "        in_height = input.size()[3]\n",
    "        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n",
    "        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n",
    "        pad_along_width = ((out_width - 1) * self.stride[0] +\n",
    "                           self.kernel_size[0] - in_width)\n",
    "        pad_along_height = ((out_height - 1) * self.stride[1] +\n",
    "                            self.kernel_size[1] - in_height)\n",
    "        pad_left = math.floor(pad_along_width / 2)\n",
    "        pad_top = math.floor(pad_along_height / 2)\n",
    "        pad_right = pad_along_width - pad_left\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "  \n",
    "  \n",
    "#단일 피쳐맵의 실험을 할때는 batch_size가 필요하지 않아서, 주석처리\n",
    "# self.batch_size = batch_size 이부분을 제외함.\n",
    "# 현재 단일 이미지의 경우 [3,512,7,7]의 shape을 가진 tensor로 출력됨.\n",
    "# num_classes값은 기학습 모델 기준으로 80개의 class 개수를 가지고 있어서\n",
    "# 80으로 넣어줄 것.\n",
    "# 현재 기준 roi_align의 output이 [num_rois,in_channels,pool_height,pool_weight]이므로\n",
    "# 생성자에서 따로 생성할 필요가 없음\n",
    "# 단, 첫 self.conv1의 입력값은 512로 맞춰줘야함.\n",
    "\n",
    "class Mask(nn.Module):\n",
    "  def __init__(self,num_rois,in_channels,pool_height,pool_weight, num_classes):\n",
    "      super(Mask, self).__init__()\n",
    "      self.num_rois = num_rois\n",
    "      self.in_channels = in_channels\n",
    "      self.pool_height = pool_height\n",
    "      self.pool_weight = pool_weight\n",
    "      self.num_classes = num_classes\n",
    "      self.padding = SamePad2d(kernel_size=3,stride=1)\n",
    "      self.conv1 = nn.Conv2d(self.in_channels, 256, kernel_size=3, stride=1)\n",
    "      self.bn1 = nn.BatchNorm2d(256, eps=0.001)\n",
    "      self.deconv = nn.ConvTranspose2d(256, 80, kernel_size=2, stride=2)\n",
    "      self.conv2 = nn.Conv2d(80, self.num_classes, kernel_size=3, stride=1)\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(self.padding(x))\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.deconv(x)\n",
    "    x = self.conv2(self.padding(x))\n",
    "    x = self.sigmoid(x)\n",
    "    p_mask = x\n",
    "    return p_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Mask(num_rois=1,in_channels=512,pool_height=7,pool_weight=7,num_classes=80)\n",
    "mask_out = mask(pooled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Mask(num_rois=1,in_channels=512,pool_height=7,pool_weight=7,num_classes=80)\n",
    "target = mask(pooled_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_roi = mask_out[0]\n",
    "# print(ck_roi)\n",
    "height = mask_out[0][2]\n",
    "weight = mask_out[0][3]\n",
    "print(height)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Loss: 0.6933982372283936\n"
     ]
    }
   ],
   "source": [
    "# 마스크 loss \n",
    "mask_prediction = mask_out  # 모델이 예측한 마스크 값\n",
    "mask_target = torch.rand_like(mask_out, dtype=torch.float)  # 랜덤한 실제 마스크 값, 실제 데이터에 따라 적절한 값을 사용해야 합니다.\n",
    "\n",
    "# BCELoss를 사용하여 마스크 손실 계산\n",
    "mask_criterion = nn.BCELoss()\n",
    "mask_loss = mask_criterion(mask_prediction, mask_target)\n",
    "\n",
    "# 마스크 손실 출력\n",
    "print(\"Mask Loss:\", mask_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_target = torch.rand_like(mask_out, dtype=torch.float)\n",
    "#fake_target 을 0~1로 정규화\n",
    "fake_target_normal = (fake_target - fake_target.min()) / (fake_target.max() - fake_target.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_target = torch.rand_like(mask_out, dtype=torch.float)\n",
    "\n",
    "# Min-Max 스케일링을 통해 [0, 1] 사이로 정규화\n",
    "fake_tg_normal = F.normalize(fake_target, dim=(2, 3), p=2)\n",
    "fake_tg_nor1 = fake_tg_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 80, 14, 14])\n",
      "torch.Size([3, 80, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print(mask_out.shape)\n",
    "print(fake_tg_nor1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,loss:0.6933\n",
      "Epoch:2,loss:0.6933\n",
      "Epoch:3,loss:0.6933\n",
      "Epoch:4,loss:0.6933\n",
      "Epoch:5,loss:0.6933\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Mask(num_rois=1,in_channels=512,pool_height=7,pool_weight=7,num_classes=80)\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "inputs = pooled_features\n",
    "mask_target = target\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    output = model(inputs)\n",
    "    mask_loss = loss(output,mask_target)\n",
    "    optimizer.zero_grad()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1)%1 ==0:\n",
    "        print(f\"Epoch:{epoch+1},loss:{mask_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"COCO_train2014_000000000030.jpg\")\n",
    "\n",
    "#target bbox좌표\n",
    "#annotation의 bbox 값은 [1,2,3,4] 이렇게 4가지의 값을 가지고있음\n",
    "#bbox[1,2,3,4]는 [left_top_x,left_top_y,width,height] 값임.\n",
    "#rectangle을 그리기위해 x1,y1,x2,y2 값이 필요하다면,\n",
    "#좌표는 annotation 기준으로 [x1,y1,x1+width,y1+height]를 [x1,y1,x2,y2]로\n",
    "#넣어서 그려주면 target bbox값을 가질 수 있음.\n",
    "\n",
    "x1,y1,x2,y2 = 204,31,458,355\n",
    "\n",
    "\n",
    "cv2.rectangle(image,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "cv2.imshow(\"image with bbox\",image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "\n",
    "roi = roi_align()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
